{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an neural network for regression\n",
    "\n",
    "- Watch this video https://lightning.ai/docs/pytorch/stable/starter/introduction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this test data with your data and adapt the code accordingly\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\n",
    "    \"https://raw.githubusercontent.com/kuennethgroup/materials_datasets/refs/heads/main/polymer_tendency_to_crystalize/polymers_tend_to_crystalize.json\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "\n",
    "# --------------------------------\n",
    "# Define a LightningModule by subclassing LightningModule\n",
    "# A LightningModule is a subclass of nn.Module\n",
    "\n",
    "\n",
    "class LitRegressor(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        # -- Define your NN\n",
    "        super().__init__()\n",
    "        input_size = 2048\n",
    "        self.l1 = nn.Sequential(nn.Linear(input_size, 300), nn.ReLU(), nn.Dropout(0.4))\n",
    "        self.l2 = nn.Sequential(nn.Linear(300, 100), nn.ReLU(), nn.Dropout(0.4))\n",
    "        self.l3 = nn.Sequential(nn.Linear(100, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -- Define how to ho forward through your NN\n",
    "        # forward defines the prediction/inference actions\n",
    "        x = self.l3(self.l2(self.l1(x)))\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # -- Define how to do a training step\n",
    "        # Split to input and output as you defined it in your dataloader\n",
    "        x, y = batch\n",
    "        # fog forward and get prediction\n",
    "        x = self.forward(x)\n",
    "        # compute loss\n",
    "        y = y.view(x.size(0), -1)\n",
    "        loss = F.mse_loss(y, x)\n",
    "        # log the loss (for plotting later)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # -- Define how to do a validation step (similar to raining)\n",
    "        x, y = batch\n",
    "        x = self.forward(x)\n",
    "        y = y.view(x.size(0), -1)\n",
    "        loss = F.mse_loss(x, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define the optimize that you want to use\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split\n",
    "train, val = train_test_split(df, shuffle=True, random_state=123)\n",
    "\n",
    "# Train\n",
    "train_fps = torch.Tensor(np.stack(train[\"fingerprint\"].values).astype(np.float32))\n",
    "train_values = torch.Tensor(np.stack(train[\"value\"].values).astype(np.float32))\n",
    "train_data = data_utils.TensorDataset(train_fps, train_values)\n",
    "train_loader = data_utils.DataLoader(train_data, batch_size=30, shuffle=False)\n",
    "\n",
    "# Validation\n",
    "val_fps = torch.Tensor(np.stack(val[\"fingerprint\"].values).astype(np.float32))\n",
    "val_values = torch.Tensor(np.stack(val[\"value\"].values).astype(np.float32))\n",
    "val_data = data_utils.TensorDataset(val_fps, val_values)\n",
    "val_loader = data_utils.DataLoader(val_data, batch_size=30, shuffle=False)\n",
    "\n",
    "# Test\n",
    "# test loader, we use the same dataset as for val (for now); no values here\n",
    "test_fps = torch.Tensor(np.stack(val[\"fingerprint\"].values).astype(np.float32))\n",
    "test_values = torch.Tensor(np.stack(val[\"value\"].values).astype(np.float32))\n",
    "test_loader = data_utils.DataLoader(test_fps, batch_size=30, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelSummary\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "save_dir = Path(\"my_pytorch_model\")\n",
    "if save_dir.exists():\n",
    "    shutil.rmtree(save_dir)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(dirpath=save_dir, save_top_k=1, monitor=\"val_loss\", verbose=True),\n",
    "    ModelSummary(max_depth=-1),\n",
    "    EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True),\n",
    "]\n",
    "\n",
    "\n",
    "regressor = LitRegressor()\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=50,\n",
    "    log_every_n_steps=1,\n",
    "    val_check_interval=1,\n",
    "    callbacks=callbacks,\n",
    "    logger=[CSVLogger(\".\")],\n",
    ")\n",
    "trainer.fit(model=regressor, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.read_csv(Path(trainer.logger.log_dir) / \"metrics.csv\")\n",
    "df_res = df_res.set_index([\"epoch\", \"step\"])\n",
    "df_res = pd.concat([df_res[\"train_loss\"].dropna(), df_res[\"val_loss\"].dropna()], axis=1)\n",
    "df_res.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "best_regressor = LitRegressor.load_from_checkpoint(list(save_dir.glob(\"*.ckpt\"))[0])\n",
    "\n",
    "trainer = L.Trainer()\n",
    "preds = trainer.predict(best_regressor, test_loader)\n",
    "preds = torch.cat(preds).squeeze().numpy()\n",
    "\n",
    "rmse = root_mean_squared_error(preds, test_values)\n",
    "r2 = r2_score(preds, test_values)\n",
    "\n",
    "ax.plot(preds, test_values, \"o\")\n",
    "ax.plot([0, 100], [0, 100], \"k--\")\n",
    "ax.set_ylabel(\"true\")\n",
    "ax.set_xlabel(\"pred\")\n",
    "print(f\"{rmse = } [%]\")\n",
    "print(f\"{r2 = }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-in-ms-st25 (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
